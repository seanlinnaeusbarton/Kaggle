{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"precision\",16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Config(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.lr = 0.001\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 1\n",
    "        self.input_size = 1\n",
    "        self.output_size = 1\n",
    "        self.num_epochs = 1000\n",
    "        self.batch_size = 10\n",
    "        self.step_size = 5\n",
    "        self.reuse = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(object):\n",
    "    def __init__(self,scope,feed,config):\n",
    "        self.config = config\n",
    "        with tf.variable_scope(scope):\n",
    "            self.feed = feed\n",
    "            self.inputs = feed.input_data\n",
    "            self.targets = feed.target_data\n",
    "            self._build_graph()\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        self._init_placeholders()\n",
    "        self._build_model()\n",
    "        self._setup_training()\n",
    "        \n",
    "    def _init_placeholders(self):\n",
    "        self.input_ph = tf.placeholder(\n",
    "            tf.float32, \n",
    "            [None, self.config.step_size, self.config.input_size],\n",
    "            name=\"input_ph\"\n",
    "        )\n",
    "        self.labels_ph = tf.placeholder(\n",
    "            tf.float32,\n",
    "            [None, self.config.step_size, self.config.output_size],\n",
    "            name=\"labels_ph\"\n",
    "        )\n",
    "        \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope(\"model\", reuse=self.config.reuse):\n",
    "            lstm_cell = tf.contrib.rnn.LSTMCell(\n",
    "                self.config.hidden_size,\n",
    "                activation=tf.nn.relu,\n",
    "                state_is_tuple=True\n",
    "            )\n",
    "            \n",
    "            batch_size = tf.shape(self.input_ph)[0]\n",
    "            \n",
    "            self.state_in = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "            \n",
    "            self.model, self.state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell,\n",
    "                self.inputs,\n",
    "                initial_state=self.state_in,\n",
    "                time_major=False\n",
    "            )\n",
    "            c,h = self.state\n",
    "            self.state_out = (c,h)\n",
    "            self.model = tf.reshape(self.model, shape=[-1,self.config.hidden_size])\n",
    "            \n",
    "            weights = tf.Variable(tf.random_uniform(\n",
    "                [self.config.hidden_size, self.config.output_size],\n",
    "                minval=-0.05, maxval=0.05\n",
    "            ))\n",
    "            biases = tf.Variable(tf.random_uniform(\n",
    "                [self.config.output_size],\n",
    "                minval=-0.05, maxval=0.05\n",
    "            ))\n",
    "            self.model = tf.nn.xw_plus_b(self.model, weights, biases, name=\"model\")\n",
    "    \n",
    "    def _setup_training(self):\n",
    "        self.loss = tf.reduce_sum(tf.square(self.model - self.targets))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.config.lr)\n",
    "        self.training = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.data.python.ops import sliding\n",
    "\n",
    "def batch_producer(raw_data, raw_targets, batch_size, step_size):\n",
    "    raw_data_tensor = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.float32)\n",
    "    raw_targets_tensor = tf.convert_to_tensor(raw_targets, name=\"raw_targets\", dtype=tf.float32)\n",
    "    \n",
    "    data_len = tf.shape(raw_data_tensor)[0]\n",
    "    batched_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data_tensor[0:batch_size*batched_len],\n",
    "                     [batch_size, batched_len])\n",
    "    targets = tf.reshape(raw_targets_tensor[0:batch_size*batched_len],\n",
    "                        [batch_size, batched_len])\n",
    "\n",
    "    data = tf.data.Dataset.from_tensor_slices(tf.transpose(data))\n",
    "    targets = tf.data.Dataset.from_tensor_slices(tf.transpose(training_label))\n",
    "    window = step_size\n",
    "    stride = 1\n",
    "\n",
    "    data = data.apply(sliding.sliding_window_batch(window,stride))\n",
    "    targets = targets.apply(sliding.sliding_window_batch(window,stride))\n",
    "\n",
    "    data_iterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes)\n",
    "    target_iterator = tf.data.Iterator.from_structure(targets.output_types, targets.output_shapes)\n",
    "    next_element = data_iterator.get_next()\n",
    "    target_element = target_iterator.get_next()\n",
    "\n",
    "    next_batch = tf.expand_dims(next_element,0)\n",
    "    next_batch = tf.transpose(next_batch)\n",
    "    next_targets = tf.expand_dims(target_element,0)\n",
    "    next_targets = tf.transpose(next_targets)\n",
    "\n",
    "    init_op = [data_iterator.make_initializer(data),target_iterator.make_initializer(targets)]\n",
    "    return next_batch, next_targets, init_op\n",
    "    \n",
    "class Data_Feed(object):\n",
    "    def __init__(self, data, targets, batch_size, step_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.step_size = step_size\n",
    "        print(len(data) // batch_size)\n",
    "        self.epoch_size = ((len(data) // batch_size)-1) // step_size\n",
    "        self.input_data, self.target_data, self.init_op = \\\n",
    "            batch_producer(data,targets, batch_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, training_targets, config):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    training_feed = Data_Feed(training_data, training_targets, config.batch_size, config.step_size)\n",
    "    model = LSTM_Model('model', training_feed, config)\n",
    "    \n",
    "    \n",
    "    print_interval = 1\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run([init_op])\n",
    "        sess.run([training_feed.init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in range(config.num_epochs):\n",
    "            lstm_state = np.zeros((config.num_layers, 2, config.batch_size, config.hidden_size))\n",
    "\n",
    "            for step in range(training_feed.epoch_size):\n",
    "                #if step % print_interval != 0:\n",
    "                #    loss, _,  lstm_state = sess.run(\\\n",
    "                #        [model.loss, model.training, model.state],\n",
    "                #        feed_dict={model.state_in : state}) \n",
    "                #else:\n",
    "                loss, _,  lstm_state = sess.run(\\\n",
    "                    [model.loss, model.training, model.state],\n",
    "                    feed_dict={model.state_in[0] : lstm_state[0][0],\n",
    "                              model.state_in[1] : lstm_state[0][1]})\n",
    "                print(\"Epoch {}, Step {}, Loss : {}\".format(\n",
    "                    epoch, step, loss\n",
    "                ))\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv(\"./train/train.csv\", dtype={'acoustic_data':np.int16, 'time_to_failure':np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "signal_train, fail_train = data['acoustic_data'].values[41:1041], \\\n",
    "                            data['time_to_failure'].values[41:1041]\n",
    "signal_test, fail_test = data['acoustic_data'].values[1041:1241], \\\n",
    "                            data['time_to_failure'].values[1041:1241]\n",
    "print(len(signal_train))\n",
    "print(len(signal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [50,1] vs. [3,5,1]\n\t [[Node: model/gradients/model/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/gradients/model/sub_grad/Shape, model/gradients/model/sub_grad/Shape_1)]]\n\nCaused by op 'model/gradients/model/sub_grad/BroadcastGradientArgs', defined at:\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-186-359e25e4cf3e>\", line 3, in <module>\n    train(signal_train, fail_train, config)\n  File \"<ipython-input-185-ef2dc15c996b>\", line 6, in train\n    model = LSTM_Model('model', training_feed, config)\n  File \"<ipython-input-175-d3223deae4e2>\", line 8, in __init__\n    self._build_graph()\n  File \"<ipython-input-175-d3223deae4e2>\", line 13, in _build_graph\n    self._setup_training()\n  File \"<ipython-input-175-d3223deae4e2>\", line 62, in _setup_training\n    self.training = self.optimizer.minimize(self.loss)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 400, in minimize\n    grad_loss=grad_loss)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 514, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 779, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 398, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 779, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 889, in _SubGrad\n    rx, ry = gen_array_ops.broadcast_gradient_args(sx, sy)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 812, in broadcast_gradient_args\n    \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'model/sub', defined at:\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 29 identical lines from previous traceback]\n  File \"<ipython-input-175-d3223deae4e2>\", line 13, in _build_graph\n    self._setup_training()\n  File \"<ipython-input-175-d3223deae4e2>\", line 60, in _setup_training\n    self.loss = tf.reduce_sum(tf.square(self.model - self.targets))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 850, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 8769, in sub\n    \"Sub\", x=x, y=y, name=name)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [50,1] vs. [3,5,1]\n\t [[Node: model/gradients/model/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/gradients/model/sub_grad/Shape, model/gradients/model/sub_grad/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [50,1] vs. [3,5,1]\n\t [[Node: model/gradients/model/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/gradients/model/sub_grad/Shape, model/gradients/model/sub_grad/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-359e25e4cf3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mLSTM_Config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfail_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-185-ef2dc15c996b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(training_data, training_targets, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                     feed_dict={model.state_in[0] : lstm_state[0][0],\n\u001b[1;32m---> 32\u001b[1;33m                               model.state_in[1] : lstm_state[0][1]})\n\u001b[0m\u001b[0;32m     33\u001b[0m                 print(\"Epoch {}, Step {}, Loss : {}\".format(\n\u001b[0;32m     34\u001b[0m                     \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [50,1] vs. [3,5,1]\n\t [[Node: model/gradients/model/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/gradients/model/sub_grad/Shape, model/gradients/model/sub_grad/Shape_1)]]\n\nCaused by op 'model/gradients/model/sub_grad/BroadcastGradientArgs', defined at:\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3215, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-186-359e25e4cf3e>\", line 3, in <module>\n    train(signal_train, fail_train, config)\n  File \"<ipython-input-185-ef2dc15c996b>\", line 6, in train\n    model = LSTM_Model('model', training_feed, config)\n  File \"<ipython-input-175-d3223deae4e2>\", line 8, in __init__\n    self._build_graph()\n  File \"<ipython-input-175-d3223deae4e2>\", line 13, in _build_graph\n    self._setup_training()\n  File \"<ipython-input-175-d3223deae4e2>\", line 62, in _setup_training\n    self.training = self.optimizer.minimize(self.loss)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 400, in minimize\n    grad_loss=grad_loss)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 514, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 779, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 398, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 779, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 889, in _SubGrad\n    rx, ry = gen_array_ops.broadcast_gradient_args(sx, sy)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 812, in broadcast_gradient_args\n    \"BroadcastGradientArgs\", s0=s0, s1=s1, name=name)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'model/sub', defined at:\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 29 identical lines from previous traceback]\n  File \"<ipython-input-175-d3223deae4e2>\", line 13, in _build_graph\n    self._setup_training()\n  File \"<ipython-input-175-d3223deae4e2>\", line 60, in _setup_training\n    self.loss = tf.reduce_sum(tf.square(self.model - self.targets))\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 850, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 8769, in sub\n    \"Sub\", x=x, y=y, name=name)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Travel\\Anaconda3\\envs\\ALLIES\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [50,1] vs. [3,5,1]\n\t [[Node: model/gradients/model/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/gradients/model/sub_grad/Shape, model/gradients/model/sub_grad/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "config =  LSTM_Config()\n",
    "\n",
    "train(signal_train, fail_train, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6  3  4  2  3 10  6  8 11 11 11  2  9  7  3  8  7  4  4]\n",
      "[[ 3  6  3  4  2  3]\n",
      " [10  6  8 11 11 11]\n",
      " [ 2  9  7  3  8  7]]\n",
      "(3, 6)\n",
      "[[1.4690999 1.4690999 1.4690999 1.4690999 1.4690999 1.4690999]\n",
      " [1.4690999 1.4690999 1.4690999 1.4690999 1.4690999 1.4690999]\n",
      " [1.4690999 1.4690999 1.4690999 1.4690999 1.4690999 1.4690999]]\n"
     ]
    }
   ],
   "source": [
    "test = signal_train[0:20]\n",
    "y = fail_train[0:20]\n",
    "batch_size = 3\n",
    "step_size = 2\n",
    "\n",
    "data_len = len(test)\n",
    "batched_len = data_len // batch_size\n",
    "epoch_size = (batched_len-1) // step_size\n",
    "\n",
    "training_data = np.reshape(test[0:batched_len*batch_size],\n",
    "                          [batch_size, batched_len])\n",
    "training_label = np.reshape(y[0:batched_len*batch_size],\n",
    "                           [batch_size, batched_len])\n",
    "\n",
    "\n",
    "\n",
    "print(test)\n",
    "print(training_data)\n",
    "print(np.shape(training_data))\n",
    "print(training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(1, 20, 1)\n",
      "[[[ 3]\n",
      "  [ 6]\n",
      "  [ 3]\n",
      "  [ 4]\n",
      "  [ 2]\n",
      "  [ 3]\n",
      "  [10]\n",
      "  [ 6]\n",
      "  [ 8]\n",
      "  [11]\n",
      "  [11]\n",
      "  [11]\n",
      "  [ 2]\n",
      "  [ 9]\n",
      "  [ 7]\n",
      "  [ 3]\n",
      "  [ 8]\n",
      "  [ 7]\n",
      "  [ 4]\n",
      "  [ 4]]]\n"
     ]
    }
   ],
   "source": [
    "test = signal_train[0:20]\n",
    "num_periods = 20\n",
    "x_data = test[0:(len(test)-(len(test) % num_periods))]\n",
    "x_batches = x_data.reshape(-1,20,1)\n",
    "print(len(test) % num_periods)\n",
    "print(x_batches.shape)\n",
    "print(x_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorSliceDataset' object has no attribute 'window'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-e0adc46fbcba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#data = data.apply(sliding.sliding_window_batch(window,stride))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msliding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msliding_window_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TensorSliceDataset' object has no attribute 'window'"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.data.python.ops import sliding\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices(training_data.T)\n",
    "targets = tf.data.Dataset.from_tensor_slices(training_label.T)\n",
    "window = 2\n",
    "stride = 1\n",
    "\n",
    "data = data.apply(sliding.sliding_window_batch(window,stride))\n",
    "targets = targets.apply(sliding.sliding_window_batch(window,stride))\n",
    "\n",
    "data_iterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes)\n",
    "target_iterator = tf.data.Iterator.from_structure(targets.output_types, targets.output_shapes)\n",
    "next_element = data_iterator.get_next()\n",
    "target_element = target_iterator.get_next()\n",
    "\n",
    "next_batch = tf.expand_dims(next_element,0)\n",
    "next_batch = tf.transpose(next_batch)\n",
    "next_targets = tf.expand_dims(target_element,0)\n",
    "next_targets = tf.transpose(next_targets)\n",
    "\n",
    "init_op = [data_iterator.make_initializer(data),target_iterator.make_initializer(targets)]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run(next_batch)\n",
    "            tar = sess.run(next_targets)\n",
    "            print(batch)\n",
    "            print(tar)\n",
    "            print(np.shape(batch))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
